on test set

--- 2016-08-03 ---
constant
  0.693071
logistic
  0.692171
logistic(bigger batches)
  0.692235 (stopped early)
smallnn(128,relu)
  0.691443

--- 2016-08-10 ---
constant
  0.693130
logistic (whitened eps=1e-5)
  0.692845
logistic whitened eps=1e-1
  0.692744
logistic whitened eps=1
  0.692868
logistic (unwhitened)
  0.692427

--- 2016-08-17 --=
logistic (minibatch size 20)
  0.690953 (0.69116 on leaderboard)
nn - 512-512-512 (tbPktIjiZkV5klJtBNOyMIhH118jqoyb)
  0.690932
nn - 21 (THGsyfezIZWMwUUJVYghihKZiFDdWrdj)
  0.690683 (0.69081 on leaderboard)
nn - 23 (Cc1CYxyTDlxumV7J5YajtONfe9U9qIh0)
  0.690641 (0.69167 on leaderboard)
autosklearn (1 hr)
  0.69125

autoencoder single 32 unit hidden layer, input dropout 0.8, no hidden dropout, rho 0.01
  6.837e-3
same, 2 unit hidden layer
  0.08313
autoencoder, 2 512 relu unit hidden layers, dropout 0.8, no sparsity penalty
  3e-4